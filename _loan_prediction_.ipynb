{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shape_AI_project_1_loan_prediction_.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4r9oBby9qwf"
      },
      "source": [
        "*The* problem:\n",
        "Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\n",
        "The Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers.\n",
        "It’s a classification problem , given information about the application we have to predict whether the they’ll be to pay the loan or not.\n",
        "We’ll start by exploratory data analysis , then preprocessing , and finally we’ll be testing different models such as Logistic regression and decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP8iQjAq94-o"
      },
      "source": [
        "# Importing Important Libraries :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3zpXxFX-AFQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLMHUGQzVDf-"
      },
      "source": [
        "#Train file will be used for training the model, i.e. our model will learn from this file. It contains all the independent variables and the target variable.\n",
        "train = pd.read_csv(\"/content/Training Data.csv\")\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpav5XqxVDib"
      },
      "source": [
        "#Test file contains all the independent variables, but not the target variable. We will apply the model to predict the target variable for the test data.\n",
        "\n",
        "test =  pd.read_csv(\"/content/Testing Data.csv\")\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX-8yXypVDmX"
      },
      "source": [
        "#Let’s make a copy of the train and test data so that even if we have to make any changes in these datasets we would not lose the original datasets.\n",
        "train_original=train.copy()\n",
        "test_original=test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnGRv96OWsuT"
      },
      "source": [
        "# Understanding the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPq8UUzAVDo0"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-rajsObXD1K"
      },
      "source": [
        "We have 12 independent variables and 1 target variable, i.e. Loan_Status in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15EYq3K9VDsF"
      },
      "source": [
        "test.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLsiWnrYXKX7"
      },
      "source": [
        "We have similar features in the test dataset as the training dataset except for the Loan_Status. We will predict the Loan_Status using the model built using the train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVAe1sKqVDtz"
      },
      "source": [
        "train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WANEU0OzXSHO"
      },
      "source": [
        "We can see there are three formats of data types:\n",
        "object: Object format means variables are categorical. Categorical variables in our dataset are Loan_ID, Gender, Married, Dependents, Education, Self_Employed, Property_Area, Loan_Status.\n",
        "int64: It represents the integer variables. ApplicantIncome is of this format.\n",
        "float64: It represents the variable that has some decimal values involved. They are also numerical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54BJQ6bXVDxb"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oruNP-c3XcCS"
      },
      "source": [
        "\n",
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-4vio2TXcEf"
      },
      "source": [
        "train['Loan_Status'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trVyG4aobeAS"
      },
      "source": [
        "train['Loan_Status'].value_counts(normalize=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXurH1UdXcIJ"
      },
      "source": [
        "train['Loan_Status'].value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mitPvIibvdg"
      },
      "source": [
        "The loan of 422(around 69%) people out of 614 were approved.\n",
        "Now, let's visualize each variable separately. Different types of variables are Categorical, ordinal, and numerical.\n",
        "Categorical features: These features have categories (Gender, Married, Self_Employed, Credit_History, Loan_Status)\n",
        "Ordinal features: Variables in categorical features having some order involved (Dependents, Education, Property_Area)\n",
        "Numerical features: These features have numerical values (ApplicantIncome, Co-applicantIncome, LoanAmount, Loan_Amount_Term)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiysFzHYXcNp"
      },
      "source": [
        "train['Gender'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpPkOpj1STLW"
      },
      "source": [
        "train['Gender'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJbu_mJFXcS1"
      },
      "source": [
        "#1. What's the ratio of Male to Female?\n",
        "Male = 489\n",
        "Female = 112\n",
        "ratio = (Male/Female)\n",
        "print(\"ratio of Male to female is= \", ratio )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de9smg_1-aOQ"
      },
      "source": [
        "train.head()#We can see that there’s some missing data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc08_OBh-jYN"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tilgj8mSAQ8w"
      },
      "source": [
        "Some variables have missing values that we’ll have to deal with , and also there seems to be some outliers for the Applicant Income , Coapplicant income and Loan Amount . We also see that about 84% applicants have a credit_history. Because the mean of Credit_History field is 0.84 and it has either (1 for having a credit history or 0 for not)\n",
        "It would be interesting to study the distribution of the numerical variables mainly the Applicant income and the loan amount. To do this we’ll use seaborn for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHYyiWF9AT3J"
      },
      "source": [
        "sns.distplot(train.ApplicantIncome,kde=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRyL6bN6Aawf"
      },
      "source": [
        "The distribution is skewed and we can notice quite a few outliers.\n",
        "Since Loan Amount has missing values , we can’t plot it directly. One solution is to drop the missing values rows then plot it, we can do this using the dropna function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18BEQxeDAbjk"
      },
      "source": [
        "sns.distplot(train.ApplicantIncome.dropna(),kde=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM3d9uBTAipi"
      },
      "source": [
        "People with better education should normally have a higher income, we can check that by plotting the education level against the income."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r37MlBeFAjxB"
      },
      "source": [
        "sns.boxplot(x='Education',y='ApplicantIncome',data=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIwSSlnEA4O-"
      },
      "source": [
        "The distributions are quite similar but we can see that the graduates have more outliers which means that the people with huge income are most likely well educated.\n",
        "Another interesting variable is credit history , to check how it affects the Loan Status we can turn it into binary then calculate it’s mean for each value of credit history . A value close to 1 indicates a high loan success rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYEKUC2NA6E3"
      },
      "source": [
        "#turn loan status into binary \n",
        "modified=train\n",
        "modified['Loan_Status']=train['Loan_Status'].apply(lambda x: 0 if x==\"N\" else 1 )\n",
        "#calculate the mean\n",
        "modified.groupby('Credit_History').mean()['Loan_Status']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaWQ2KBgBK7S"
      },
      "source": [
        "People with a credit history a way more likely to pay their loan, 0.07 vs 0.79 . This means that credit history will be an influential variable in our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhlUaqx7o1ym"
      },
      "source": [
        "############ Count number of Categorical and Numerical Columns ######################\n",
        "train_df = train.drop(columns=['Loan_ID']) ## Dropping Loan ID\n",
        "categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']\n",
        "#categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Loan_Amount_Term']\n",
        "\n",
        "print(categorical_columns)\n",
        "numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
        "print(numerical_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AF1Q2S_BYQT"
      },
      "source": [
        "### Data Visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig,axes = plt.subplots(4,2,figsize=(12,15))\n",
        "for idx,cat_col in enumerate(categorical_columns):\n",
        "    row,col = idx//2,idx%2\n",
        "    sns.countplot(x=cat_col,data=train_df,hue='Loan_Status',ax=axes[row,col])\n",
        "\n",
        "\n",
        "plt.subplots_adjust(hspace=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ41gCUkpcPk"
      },
      "source": [
        "# Plots above convey following things about the dataset:\n",
        " \n",
        "\n",
        "1.   Loan Approval Status: About 2/3rd of applicants have been granted loan.\n",
        "2.   Sex: There are more Men than Women (approx. 3x)\n",
        "Martial Status: 2/3rd of the population in the dataset is Married \n",
        "3.  Married applicants are more likely to be granted loans.\n",
        "4. Dependents: Majority of the population have zero dependents and are also likely to accepted for loan.\n",
        "5. Education: About 5/6th of the population is Graduate and graduates have higher propotion of loan approval\n",
        "6. Employment: 5/6th of population is not self employed.\n",
        "7. Property Area: More applicants from Semi-urban and also likely to be granted loans.\n",
        "Applicant with credit history are far more likely to be accepted.\n",
        "8. Loan Amount Term: Majority of the loans taken are for 360 Months (30 years)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn0wo3sMoyGL"
      },
      "source": [
        "### Data Visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig,axes = plt.subplots(4,2,figsize=(12,15))\n",
        "for idx,cat_col in enumerate(categorical_columns):\n",
        "    row,col = idx//2,idx%2\n",
        "    sns.countplot(x=cat_col,data=train_df,hue='Married',ax=axes[row,col])\n",
        "\n",
        "\n",
        "plt.subplots_adjust(hspace=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNDkev3br7Dy"
      },
      "source": [
        "According to plots :\n",
        "1. more than 300 males are married and more than 100 males are non married\n",
        "2. very less number(below 100) of women  are married.\n",
        "Now, let's also analyze Numerical Columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqtjnMVLvMEs"
      },
      "source": [
        "fig,axes = plt.subplots(1,3,figsize=(17,5))\n",
        "for idx,cat_col in enumerate(numerical_columns):\n",
        "    sns.boxplot(y=cat_col,data=train_df,x='Loan_Status',ax=axes[idx])\n",
        "\n",
        "print(train_df[numerical_columns].describe())\n",
        "plt.subplots_adjust(hspace=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3IAh3qqvpsM"
      },
      "source": [
        "For Numercical Columns, there is no significant relation to Loan approval status.\n",
        "\n",
        "\n",
        "\n",
        "Preprocessing Data:\n",
        "Input data needs to be pre-processed before we feed it to model. Following things need to be taken care:\n",
        "\n",
        "Encoding Categorical Features.\n",
        "Imputing missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAOvw7Fxvo-c"
      },
      "source": [
        "#### Encoding categrical Features: ##########\n",
        "train_df_encoded = pd.get_dummies(train_df,drop_first=True)\n",
        "train_df_encoded.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtQbBwvU0ZR5"
      },
      "source": [
        "Now let’s look at the correlation between all the numerical variables. We will use the heat map to visualize the correlation. Heatmaps visualize data through variations in coloring. The variables with darker color means their correlation is more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-wEYkOB0Z3h"
      },
      "source": [
        "matrix = train.corr()\n",
        "f, ax = plt.subplots(figsize=(9,6))\n",
        "sns.heatmap(matrix,vmax=.8,square=True,cmap=\"BuPu\", annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neAqqB1Q0320"
      },
      "source": [
        "We see that the most correlate variables are (ApplicantIncome — LoanAmount) and (Credit_History — Loan_Status). LoanAmount is also correlated with CoapplicantIncome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdtkAA2w07xL"
      },
      "source": [
        "# Missing value imputation\n",
        "Let’s list out feature-wise count of missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXlwLcj_095v"
      },
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuXxDpaH1BYU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRHEyqKG1DqU"
      },
      "source": [
        "There are missing values in Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term, and Credit_History features.\n",
        "We will treat the missing values in all the features one by one.\n",
        "We can consider these methods to fill the missing values:\n",
        "1. For numerical variables: imputation \n",
        "using mean or median\n",
        "2. For categorical variables: imputation using mode\n",
        "There are very few missing values in Gender, Married, Dependents, Credit_History, and Self_Employed features so we can fill them using the mode of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCoICHXW1L8g"
      },
      "source": [
        "train['Gender'].fillna(train['Gender'].mode()[0], inplace=True)\n",
        "train['Married'].fillna(train['Married'].mode()[0], inplace=True)\n",
        "train['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True)\n",
        "train['Self_Employed'].fillna(train['Self_Employed'].mode()[0], inplace=True)\n",
        "train['Credit_History'].fillna(train['Credit_History'].mode()[0], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq4U7lSU2Pui"
      },
      "source": [
        "train['Loan_Amount_Term'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhyOaAfQ23d4"
      },
      "source": [
        "train['Gender'].fillna(train['Gender'].mode()[0], inplace=True)\n",
        "train['Married'].fillna(train['Married'].mode()[0], inplace=True)\n",
        "train['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True)\n",
        "train['Self_Employed'].fillna(train['Self_Employed'].mode()[0], inplace=True)\n",
        "train['Credit_History'].fillna(train['Credit_History'].mode()[0], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB_bwF273tZI"
      },
      "source": [
        "train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF3dxPzF4AM-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV0JsQpF4IYs"
      },
      "source": [
        "# Decision Tree Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487mhIzJ4MNd"
      },
      "source": [
        "########## Split Features and Target Varible ############\n",
        "X = train_df_encoded.drop(columns='Loan_Status_Y')\n",
        "y = train_df_encoded['Loan_Status_Y']\n",
        "\n",
        "################# Splitting into Train -Test Data #######\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)\n",
        "############### Handling/Imputing Missing values #############\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(strategy='mean')\n",
        "imp_train = imp.fit(X_train)\n",
        "X_train = imp_train.transform(X_train)\n",
        "X_test_imp = imp_train.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq8F8IeZ4qfX"
      },
      "source": [
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "\n",
        "tree_clf = DecisionTreeClassifier()\n",
        "tree_clf.fit(X_train,y_train)\n",
        "y_pred = tree_clf.predict(X_train)\n",
        "print(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\n",
        "print(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n",
        "\n",
        "print(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\n",
        "print(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPbPy5th4vjx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEkVYvrR5CNx"
      },
      "source": [
        "Overfitting Problem\n",
        "We can see from above metrics that Training Accuracy > Test Accuracy with default settings of Decision Tree classifier. Hence, model is overfit. We will try some Hyper-parameter tuning and see if it helps.\n",
        "\n",
        "First let's try tuning 'Max_Depth' of tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEc10SMB5Cza"
      },
      "source": [
        "training_accuracy = []\n",
        "val_accuracy = []\n",
        "training_f1 = []\n",
        "val_f1 = []\n",
        "tree_depths = []\n",
        "\n",
        "for depth in range(1,20):\n",
        "    tree_clf = DecisionTreeClassifier(max_depth=depth)\n",
        "    tree_clf.fit(X_train,y_train)\n",
        "    y_training_pred = tree_clf.predict(X_train)\n",
        "\n",
        "    training_acc = accuracy_score(y_train,y_training_pred)\n",
        "    train_f1 = f1_score(y_train,y_training_pred)\n",
        "    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n",
        "    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n",
        "    \n",
        "    training_accuracy.append(training_acc)\n",
        "    val_accuracy.append(val_mean_accuracy)\n",
        "    training_f1.append(train_f1)\n",
        "    val_f1.append(val_mean_f1)\n",
        "    tree_depths.append(depth)\n",
        "    \n",
        "\n",
        "Tuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\n",
        "Tuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n",
        "\n",
        "plot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\n",
        "fig,ax = plt.subplots(figsize=(15,5))\n",
        "sns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVzgsoyx5FXh"
      },
      "source": [
        "From above graph, we can conclude that keeping 'Max_Depth' = 3 will yield optimum Test accuracy and F1 score Optimum Test Accuracy ~ 0.805; Optimum F1 Score: ~0.7\n",
        "\n",
        "Visulazing Decision Tree with Max Depth = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm6yjaLB5X8w"
      },
      "source": [
        "import graphviz \n",
        "from sklearn import tree\n",
        "\n",
        "tree_clf = tree.DecisionTreeClassifier(max_depth = 3)\n",
        "tree_clf.fit(X_train,y_train)\n",
        "dot_data = tree.export_graphviz(tree_clf,feature_names = X.columns.tolist())\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUpd9R1X5dhQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjJ_NfHP5fvZ"
      },
      "source": [
        "From above tree, we could see that some of the leafs have less than 5 samples hence our classifier might overfit. We can sweep hyper-parameter 'min_samples_leaf' to further improve test accuracy by keeping max_depth to 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGpQin_C5gRM"
      },
      "source": [
        "training_accuracy = []\n",
        "val_accuracy = []\n",
        "training_f1 = []\n",
        "val_f1 = []\n",
        "min_samples_leaf = []\n",
        "import numpy as np\n",
        "for samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf \n",
        "    tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = samples_leaf)\n",
        "    tree_clf.fit(X_train,y_train)\n",
        "    y_training_pred = tree_clf.predict(X_train)\n",
        "\n",
        "    training_acc = accuracy_score(y_train,y_training_pred)\n",
        "    train_f1 = f1_score(y_train,y_training_pred)\n",
        "    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n",
        "    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n",
        "    \n",
        "    training_accuracy.append(training_acc)\n",
        "    val_accuracy.append(val_mean_accuracy)\n",
        "    training_f1.append(train_f1)\n",
        "    val_f1.append(val_mean_f1)\n",
        "    min_samples_leaf.append(samples_leaf)\n",
        "    \n",
        "\n",
        "Tuning_min_samples_leaf = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Min_Samples_leaf\": min_samples_leaf }\n",
        "Tuning_min_samples_leaf_df = pd.DataFrame.from_dict(Tuning_min_samples_leaf)\n",
        "\n",
        "plot_df = Tuning_min_samples_leaf_df.melt('Min_Samples_leaf',var_name='Metrics',value_name=\"Values\")\n",
        "fig,ax = plt.subplots(figsize=(15,5))\n",
        "sns.pointplot(x=\"Min_Samples_leaf\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDOQRvVp5wDY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBeRQ_o512E"
      },
      "source": [
        "From above plot, we will choose Min_Samples_leaf to 35 to improve test accuracy.\n",
        "\n",
        "Let's use this Decision Tree classifier on unseen test data and evaluate Test Accuracy, F1 Score and Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMEVZNf_52hj"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = 35)\n",
        "tree_clf.fit(X_train,y_train)\n",
        "y_pred = tree_clf.predict(X_test_imp)\n",
        "print(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "print(\"Test F1 Score: \",f1_score(y_test,y_pred))\n",
        "print(\"Confusion Matrix on Test Data\")\n",
        "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOkGNLua6CSW"
      },
      "source": [
        "Mis-classifications\n",
        "It can be seen that majority of the misclassifications are happening because of Loan Reject applicants being classified as Accept.\n",
        "\n",
        "Let's look into Random Forest Classifier if it can reduce mis-classifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGsQG3Qx6Dra"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbHISWy6GPM"
      },
      "source": [
        "#  Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJlIzE3D6Kgt"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100,max_depth=3,min_samples_leaf = 10)\n",
        "rf_clf.fit(X_train,y_train)\n",
        "y_pred = rf_clf.predict(X_train)\n",
        "print(\"Train F1 Score \", f1_score(y_train,y_pred))\n",
        "print(\"Train Accuracy \", accuracy_score(y_train,y_pred))\n",
        "\n",
        "print(\"Validation Mean F1 Score: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\n",
        "print(\"Validation Mean Accuracy: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='accuracy').mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCWp-q3h6NDD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WA2duuo6PhH"
      },
      "source": [
        "# Random Forest: Test Data Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRGFsMBn6Q6f"
      },
      "source": [
        "y_pred = rf_clf.predict(X_test_imp)\n",
        "print(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "print(\"Test F1 Score: \",f1_score(y_test,y_pred))\n",
        "print(\"Confusion Matrix on Test Data\")\n",
        "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leOnNyrP6Si-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4GscVnc6adP"
      },
      "source": [
        "Random Forest gives same results as Decision Tree Classifier. Finally, we will try Logistic Regression Model by sweeping threshold values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoiMy1Mt6WSv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg8y5fkT6eNx"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTs8BWwz7YVB"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "train_accuracies = []\n",
        "train_f1_scores = []\n",
        "test_accuracies = []\n",
        "test_f1_scores = []\n",
        "thresholds = []\n",
        "\n",
        "#for thresh in np.linspace(0.1,0.9,8): ## Sweeping from threshold of 0.1 to 0.9\n",
        "for thresh in np.arange(0.1,0.9,0.1): ## Sweeping from threshold of 0.1 to 0.9\n",
        "    logreg_clf = LogisticRegression(solver='liblinear')\n",
        "    logreg_clf.fit(X_train,y_train)\n",
        "    \n",
        "    y_pred_train_thresh = logreg_clf.predict_proba(X_train)[:,1]\n",
        "    y_pred_train = (y_pred_train_thresh > thresh).astype(int)\n",
        "\n",
        "    train_acc = accuracy_score(y_train,y_pred_train)\n",
        "    train_f1 = f1_score(y_train,y_pred_train)\n",
        "    \n",
        "    y_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\n",
        "    y_pred_test = (y_pred_test_thresh > thresh).astype(int) \n",
        "    \n",
        "    test_acc = accuracy_score(y_test,y_pred_test)\n",
        "    test_f1 = f1_score(y_test,y_pred_test)\n",
        "    \n",
        "    train_accuracies.append(train_acc)\n",
        "    train_f1_scores.append(train_f1)\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_f1_scores.append(test_f1)\n",
        "    thresholds.append(thresh)\n",
        "    \n",
        "    \n",
        "Threshold_logreg = {\"Training Accuracy\": train_accuracies, \"Test Accuracy\": test_accuracies, \"Training F1\": train_f1_scores, \"Test F1\":test_f1_scores, \"Decision Threshold\": thresholds }\n",
        "Threshold_logreg_df = pd.DataFrame.from_dict(Threshold_logreg)\n",
        "\n",
        "plot_df = Threshold_logreg_df.melt('Decision Threshold',var_name='Metrics',value_name=\"Values\")\n",
        "fig,ax = plt.subplots(figsize=(15,5))\n",
        "sns.pointplot(x=\"Decision Threshold\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LcXL8WO7Y7t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f49EKo1P7czg"
      },
      "source": [
        "Logistic Regression does slightly better than Decision Tree and Random Forest.\n",
        "Based on the above Test/Train curves, we can keep threshold to 0.4.\n",
        "Now Finally let's look at Logistic Regression Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbIYP4N87dL7"
      },
      "source": [
        "thresh = 0.4 ### Threshold chosen from above Curves\n",
        "y_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\n",
        "y_pred = (y_pred_test_thresh > thresh).astype(int) \n",
        "print(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\n",
        "print(\"Test F1 Score: \",f1_score(y_test,y_pred))\n",
        "print(\"Confusion Matrix on Test Data\")\n",
        "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-8QlQfa7gH9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lriHLkL7jSE"
      },
      "source": [
        "Logistic Regression Confusion matrix is very similar to Decision Tree and Random Forest Classifier. In this analysis, we did extensive analysis of input data and were able to achieve Test Accuracy of 86 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWQORVon7m9a"
      },
      "source": [
        "#Conclusion:\n",
        "I’ve gone through a good portion of the data science pipe line in thisproject, namely EDA , preprocessing and modeling and I’ve used essential classification models such as Logistic regression , Decision tree and Random forests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QJUHhvU8U-t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}